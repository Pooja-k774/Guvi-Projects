{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus='''I'm Pooja and I'm here to teach NLP to the students and they seem eager to learn this topic.\n",
    "It's a four hour lecture which occurs on every Tuesday'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm Pooja and I'm here to teach NLP to the students and they seem eager to learn this topic.\",\n",
       " \"It's a four hour lecture which occurs on every Tuesday\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Beans. I was trying to explain to somebody as we were flying in, that’s corn.  That’s beans. And they were very impressed at my agricultural knowledge. Please give it up for Amaury once again for that outstanding introduction. I have a bunch of good friends here today, including somebody who I served with, who is one of the finest senators in the country, and we’re lucky to have him, your Senator, Dick Durbin is here. I also noticed, by the way, former Governor Edgar here, who I haven’t seen in a long time, and somehow he has not aged and I have. And it’s great to see you, Governor. I want to thank President Killeen and everybody at the U of I System for making it possible for me to be here today. And I am deeply honored at the Paul Douglas Award that is being given to me. He is somebody who set the path for so much outstanding public service here in Illinois. Now, I want to start by addressing the elephant in the room. I know people are still wondering why I didn’t speak at the commencement.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python3 code for preprocessing text \n",
    "import nltk \n",
    "import re \n",
    "import numpy as np \n",
    "  \n",
    "# execute the text here as : \n",
    "# text = \"\"\" # place text here  \"\"\" \n",
    "dataset = nltk.sent_tokenize(text) \n",
    "for i in range(len(dataset)): \n",
    "    dataset[i] = dataset[i].lower() \n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i]) \n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i know people are still wondering why i didn t speak at the commencement '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus = 'People should learn to respect others. Give and take respect policy shoul be followed'\n",
    "\n",
    "sent=sent_tokenize(Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['People should learn to respect others.', 'Give and take respect policy shoul be followed']\n"
     ]
    }
   ],
   "source": [
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "\n",
    "Stem=ps.stem('Establishment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "establish\n"
     ]
    }
   ],
   "source": [
    "print(Stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting :-> connect\n",
      "establishing :-> establish\n",
      "requirement :-> requir\n",
      "punishment :-> punish\n"
     ]
    }
   ],
   "source": [
    "words= ['Connecting','Establishing','Requirement','Punishment']\n",
    "lower= [word.lower() for word in words]\n",
    "for w in lower:\n",
    "    stemmedword=ps.stem(w)\n",
    "    #sent_tokenize(w)\n",
    "    print(w,\":->\", stemmedword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus='''The stock market is highly volatile these days. \n",
    "Many investors are looking for the best opportunities, but they should be cautious before making any decisions. \n",
    "********It is important to analyze trends and patterns before investing large amounts of money in stocks.'''\n",
    "\n",
    "sw=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#OneHotEncoding in Machine Learning, It converts categorical data into binary form.It functions by representing each category in feature as binary vectors of 1s an 0s with vector size equivalent to number of potential categories.\n",
    "\n",
    "#'''In Sentiment analysis assignment for example, we describe each word in a sentence as One Hot Encoded vector and use these vector as input to forecast the sentiment of the sentence.\n",
    "\n",
    "#The quick brown fox jumps over the lazy dog\n",
    "#She sells seashells by the seashore.\n",
    "#Peter Piper picked a peck of pickled pipers\n",
    "\n",
    "# Step 1: Each word in the phrase should be used as single compressed vectors\n",
    "# Step 2: Determine the distinct word in the sentence to calculate the number of potential categories\n",
    "# Step 3: In the above category, OneHotEncoding will show 18 potential categories.\n",
    "\n",
    "import numpy as np\n",
    "#Define a corpus\n",
    "\n",
    "corpus=[\"The The quick brown fox jumps over the lazy dog\",\n",
    "            \"She sells seashells by the seashore.\",\n",
    "         \"Peter Piper picked a peck of pickled pipers\"]\n",
    "\n",
    "#Create a set of unique words in the corpus\n",
    "\n",
    "unique_words = set()\n",
    "for sentence in corpus:\n",
    "    #Split the sentence and convert the word to lower\n",
    "    for word in sentence.split():\n",
    "        unique_words.add(word.lower())\n",
    "    #Create an index to map each unique word to an index\n",
    "    #Create a dictionary to map each word to an index\n",
    "word_to_index = {}\n",
    "for i, word in enumerate(unique_words):\n",
    "        word_to_index[word] = i\n",
    "    \n",
    "    #Create one hot encoded vector for each word in the corpus\n",
    "one_hot_vectors=[]\n",
    "\n",
    "for sentence in corpus:\n",
    "        sentence_vectors = []\n",
    "        for word in sentence.split():\n",
    "            vector=np.zeros(len(unique_words))\n",
    "            vector[word_to_index[word.lodwer()]] = 1\n",
    "            sentence_vectors.append(vector)\n",
    "        one_hot_vectors.append(sentence_vectors)\n",
    "\n",
    "#Print the one hot encoded vectors \n",
    "for vector in one_hot_vectors[0]:\n",
    "    print(vector)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the corpus of text\n",
    "corpus = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "\t\"She sells seashells by the seashore.\",\n",
    "\t\"Peter Piper picked a peck of pickled peppers.\"]\n",
    "\n",
    "# Create a set of unique words in the corpus\n",
    "unique_words = set()\n",
    "for sentence in corpus:\n",
    "\tfor word in sentence.split():\n",
    "\t\tunique_words.add(word.lower())\n",
    "\n",
    "# Create a dictionary to map each\n",
    "# unique word to an index\n",
    "word_to_index = {}\n",
    "for i, word in enumerate(unique_words):\n",
    "\tword_to_index[word] = i\n",
    "\n",
    "# Create one-hot encoded vectors for\n",
    "# each word in the corpus\n",
    "one_hot_vectors = []\n",
    "for sentence in corpus:\n",
    "\tsentence_vectors = []\n",
    "\tfor word in sentence.split():\n",
    "\t\tvector = np.zeros(len(unique_words))\n",
    "\t\tvector[word_to_index[word.lower()]] = 1\n",
    "\t\tsentence_vectors.append(vector)\n",
    "\tone_hot_vectors.append(sentence_vectors)\n",
    "\n",
    "# Print the one-hot encoded vectors \n",
    "# for the first sentence\n",
    "print(\"One-hot encoded vectors for the first sentence:\")\n",
    "for vector in one_hot_vectors[0]:\n",
    "\tprint(vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word embeddings\n",
    "#In NLP, word embedding is a term used for the representation of words for text analysis, typically in the\n",
    "#form of real valued vectors that encodes the meaning of words for text analysis, typically in the form of real-valued\n",
    "#vectors that encodes the meaning of the word such that the words that are closer in vector space are expected to be \n",
    "# similar in meaning. OHE, BOW and TF-IDF are all part of word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Word2Vec\n",
    " #Word2Vec is a technique for Natural Language processing published in 2013. The Word2Vec is a algorithm uses a neural network moel to learn words associated from\n",
    "#large corpus of text. Once trained, such a model can detect synonyms words or suggest additional words for a partial sentence.\n",
    "#As the name implies, word2vec represents each distinct words with a partial list of numbers called as vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
