{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading all: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus='''I'm Pooja and I'm here to learn NLP from the mentors and people seem eager to learn this topic.\n",
    "There is lot research going on for the same'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm Pooja and I'm here to learn NLP from the mentors and people seem eager to learn this topic.\",\n",
       " 'There is lot research going on for the same']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''I am stuying in thir sem.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python3 code for preprocessing text \n",
    "import nltk \n",
    "import re \n",
    "import numpy as np \n",
    "  \n",
    "# execute the text here as : \n",
    "# text = \"\"\" # place text here  \"\"\" \n",
    "dataset = nltk.sent_tokenize(text) \n",
    "for i in range(len(dataset)): \n",
    "    dataset[i] = dataset[i].lower() \n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i]) \n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am stuying in thir sem '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus = 'People should learn to respect others. Give and take respect policy shoul be followed'\n",
    "\n",
    "sent=sent_tokenize(Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['People should learn to respect others.', 'Give and take respect policy shoul be followed']\n"
     ]
    }
   ],
   "source": [
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "\n",
    "Stem=ps.stem('Establishment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "establish\n"
     ]
    }
   ],
   "source": [
    "print(Stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting :-> connect\n",
      "establishing :-> establish\n",
      "requirement :-> requir\n",
      "punishment :-> punish\n"
     ]
    }
   ],
   "source": [
    "words= ['Connecting','Establishing','Requirement','Punishment']\n",
    "lower= [word.lower() for word in words]\n",
    "for w in lower:\n",
    "    stemmedword=ps.stem(w)\n",
    "    #sent_tokenize(w)\n",
    "    print(w,\":->\", stemmedword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus='''The stock market is highly volatile these days. \n",
    "Many investors are looking for the best opportunities, but they should be cautious before making any decisions. \n",
    "********It is important to analyze trends and patterns before investing large amounts of money in stocks.'''\n",
    "\n",
    "sw=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#OneHotEncoding in Machine Learning, It converts categorical data into binary form.It functions by representing each category in feature as binary vectors of 1s an 0s with vector size equivalent to number of potential categories.\n",
    "\n",
    "#'''In Sentiment analysis assignment for example, we describe each word in a sentence as One Hot Encoded vector and use these vector as input to forecast the sentiment of the sentence.\n",
    "\n",
    "#The quick brown fox jumps over the lazy dog\n",
    "#She sells seashells by the seashore.\n",
    "#Peter Piper picked a peck of pickled pipers\n",
    "\n",
    "# Step 1: Each word in the phrase should be used as single compressed vectors\n",
    "# Step 2: Determine the distinct word in the sentence to calculate the number of potential categories\n",
    "# Step 3: In the above category, OneHotEncoding will show 18 potential categories.\n",
    "\n",
    "import numpy as np\n",
    "#Define a corpus\n",
    "\n",
    "corpus=[\"The The quick brown fox jumps over the lazy dog\",\n",
    "            \"She sells seashells by the seashore.\",\n",
    "         \"Peter Piper picked a peck of pickled pipers\"]\n",
    "\n",
    "#Create a set of unique words in the corpus\n",
    "\n",
    "unique_words = set()\n",
    "for sentence in corpus:\n",
    "    #Split the sentence and convert the word to lower\n",
    "    for word in sentence.split():\n",
    "        unique_words.add(word.lower())\n",
    "    #Create an index to map each unique word to an index\n",
    "    #Create a dictionary to map each word to an index\n",
    "word_to_index = {}\n",
    "for i, word in enumerate(unique_words):\n",
    "        word_to_index[word] = i\n",
    "    \n",
    "    #Create one hot encoded vector for each word in the corpus\n",
    "one_hot_vectors=[]\n",
    "\n",
    "for sentence in corpus:\n",
    "        sentence_vectors = []\n",
    "        for word in sentence.split():\n",
    "            vector=np.zeros(len(unique_words))\n",
    "            vector[word_to_index[word.lodwer()]] = 1\n",
    "            sentence_vectors.append(vector)\n",
    "        one_hot_vectors.append(sentence_vectors)\n",
    "\n",
    "#Print the one hot encoded vectors \n",
    "for vector in one_hot_vectors[0]:\n",
    "    print(vector)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded vectors for the first sentence:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the corpus of text\n",
    "corpus = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "\t\"She sells seashells by the seashore.\",\n",
    "\t\"Peter Piper picked a peck of pickled peppers.\"]\n",
    "\n",
    "# Create a set of unique words in the corpus\n",
    "unique_words = set()\n",
    "for sentence in corpus:\n",
    "\tfor word in sentence.split():\n",
    "\t\tunique_words.add(word.lower())\n",
    "\n",
    "# Create a dictionary to map each\n",
    "# unique word to an index\n",
    "word_to_index = {}\n",
    "for i, word in enumerate(unique_words):\n",
    "\tword_to_index[word] = i\n",
    "\n",
    "# Create one-hot encoded vectors for\n",
    "# each word in the corpus\n",
    "one_hot_vectors = []\n",
    "for sentence in corpus:\n",
    "\tsentence_vectors = []\n",
    "\tfor word in sentence.split():\n",
    "\t\tvector = np.zeros(len(unique_words))\n",
    "\t\tvector[word_to_index[word.lower()]] = 1\n",
    "\t\tsentence_vectors.append(vector)\n",
    "\tone_hot_vectors.append(sentence_vectors)\n",
    "\n",
    "# Print the one-hot encoded vectors \n",
    "# for the first sentence\n",
    "print(\"One-hot encoded vectors for the first sentence:\")\n",
    "for vector in one_hot_vectors[0]:\n",
    "\tprint(vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disavantages of OHE\n",
    "#1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word embeddings\n",
    "#In NLP, word embedding is a term used for the representation of words for text analysis, typically in the\n",
    "#form of real valued vectors that encodes the meaning of words for text analysis, typically in the form of real-valued\n",
    "#vectors that encodes the meaning of the word such that the words that are closer in vector space are expected to be \n",
    "# similar in meaning. OHE, BOW and TF-IDF are all part of word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Word2Vec\n",
    " #Word2Vec is a technique for Natural Language processing published in 2013. The Word2Vec is a algorithm uses a neural network moel to learn words associated from\n",
    "#large corpus of text. Once trained, such a model can detect synonyms words or suggest additional words for a partial sentence.\n",
    "#As the name implies, word2vec represents each distinct words with a partial list of numbers called as vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAG OF WORDS EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'learning']\n",
      "['learning', 'is', 'a', 'good', 'practice']\n",
      "['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'is', 'a', 'good', 'practice']\n",
      "['welcome', 'great', 'learning', 'now', 'start', 'good', 'practice']\n",
      "[1, 1, 2, 1, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def vectorize(tokens):\n",
    "    ''' This function takes list of words in a sentence as input \n",
    "    and returns a vector of size of filtered_vocab.It puts 0 if the \n",
    "    word is not present in tokens and count of token if present.'''\n",
    "    vector=[]\n",
    "    for w in filtered_vocab:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector\n",
    "def unique(sequence):\n",
    "    '''This functions returns a list in which the order remains \n",
    "    same and no item repeats.Using the set() function does not \n",
    "    preserve the original ordering,so i didnt use that instead'''\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "#create a list of stopwords.You can import stopwords from nltk too\n",
    "stopwords=[\"to\",\"is\",\"a\"]\n",
    "#list of special characters.You can use regular expressions too\n",
    "special_char=[\",\",\":\",\" \",\";\",\".\",\"?\"]\n",
    "#Write the sentences in the corpus,in our case, just two \n",
    "string1=\"Welcome to Great Learning , Now start learning\"\n",
    "string2=\"Learning is a good practice\"\n",
    "#convert them to lower case\n",
    "string1=string1.lower()\n",
    "string2=string2.lower()\n",
    "#split the sentences into tokens\n",
    "tokens1=string1.split()\n",
    "tokens2=string2.split()\n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "#create a vocabulary list\n",
    "vocab=unique(tokens1+tokens2)\n",
    "print(vocab)\n",
    "#filter the vocabulary list\n",
    "filtered_vocab=[]\n",
    "for w in vocab: \n",
    "    if w not in stopwords and w not in special_char: \n",
    "        filtered_vocab.append(w)\n",
    "print(filtered_vocab)\n",
    "#convert sentences into vectords\n",
    "vector1=vectorize(tokens1)\n",
    "print(vector1)\n",
    "vector2=vectorize(tokens2)\n",
    "print(vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python3 code for preprocessing text \n",
    "import nltk \n",
    "import re \n",
    "import numpy as np \n",
    "  \n",
    "# execute the text here as : \n",
    "# text = \"\"\" # place text here  \"\"\" \n",
    "dataset = nltk.sent_tokenize(text) \n",
    "for i in range(len(dataset)): \n",
    "    dataset[i] = dataset[i].lower() \n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i]) \n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beans ', 'i was trying to explain to somebody as we were flying in that s corn ', 'that s beans ', 'and they were very impressed at my agricultural knowledge ', 'please give it up for amaury once again for that outstanding introduction ', 'i have a bunch of good friends here today including somebody who i served with who is one of the finest senators in the country and we re lucky to have him your senator dick durbin is here ', 'i also noticed by the way former governor edgar here who i haven t seen in a long time and somehow he has not aged and i have ', 'and it s great to see you governor ', 'i want to thank president killeen and everybody at the u of i system for making it possible for me to be here today ', 'and i am deeply honored at the paul douglas award that is being given to me ', 'he is somebody who set the path for so much outstanding public service here in illinois ', 'now i want to start by addressing the elephant in the room ', 'i know people are still wondering why i didn t speak at the commencement ']\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model \n",
    "word2count = {} \n",
    "for data in dataset: \n",
    "    words = nltk.word_tokenize(data) \n",
    "    for word in words: \n",
    "        if word not in word2count.keys(): \n",
    "            word2count[word] = 1\n",
    "        else: \n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our model, we have a total of 118 words. However when processing large texts, the number of words could reach millions. We do not need to use all those words. Hence, we select a particular number of most frequently used words. To implement this we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq \n",
    "freq_words = heapq.nlargest(100, word2count, key=word2count.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] \n",
    "for data in dataset: \n",
    "    vector = [] \n",
    "    for word in freq_words: \n",
    "        if word in nltk.word_tokenize(data): \n",
    "            vector.append(1) \n",
    "        else: \n",
    "            vector.append(0) \n",
    "    X.append(vector) \n",
    "X = np.asarray(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf(i,j)= n(i,j)/sum(n(i,j))\n",
    "# IDF = 1+log(N/dN)\n",
    "\n",
    "#N = Total number of documents in the dataset\n",
    "#dN = total number of documents in which nth word occur\n",
    "\n",
    "#TF-IDF=TF*IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # For numerical operations - mean\n",
    "import math # For mathematical operations - log\n",
    "import fitz # from PyMuPDF for pdf file reading\n",
    "import nltk # For natural language text processing\n",
    "\n",
    "nltk.download('punkt') # For tokenization\n",
    "nltk.download('stopwords') # For removing the stopwords after tokenization\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize # For tokenizing the text\n",
    "from nltk.corpus import stopwords # For removing the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "text = '''I am is was were fruits, vegetables'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: I am is was were fruits, vegetables\n",
      "After Stopword Removal: fruits, vegetables\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "words = text.split()\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Original:\", text)\n",
    "print(\"After Stopword Removal:\", \" \" .join(filtered_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\acer\\desktop\\vm\\.venv\\lib\\site-packages (4.3.3)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\acer\\desktop\\vm\\.venv\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\acer\\desktop\\vm\\.venv\\lib\\site-packages (from gensim) (1.13.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\acer\\desktop\\vm\\.venv\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\acer\\desktop\\vm\\.venv\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gensim library is use in Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"Cell_Phones_and_Accessories_5.json\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A30TL5EWN6DFXT</td>\n",
       "      <td>120401325X</td>\n",
       "      <td>christina</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>They look good and stick good! I just don't li...</td>\n",
       "      <td>4</td>\n",
       "      <td>Looks Good</td>\n",
       "      <td>1400630400</td>\n",
       "      <td>05 21, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASY55RVNIL0UD</td>\n",
       "      <td>120401325X</td>\n",
       "      <td>emily l.</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>These stickers work like the review says they ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Really great product.</td>\n",
       "      <td>1389657600</td>\n",
       "      <td>01 14, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2TMXE2AFO7ONB</td>\n",
       "      <td>120401325X</td>\n",
       "      <td>Erica</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>These are awesome and make my phone look so st...</td>\n",
       "      <td>5</td>\n",
       "      <td>LOVE LOVE LOVE</td>\n",
       "      <td>1403740800</td>\n",
       "      <td>06 26, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AWJ0WZQYMYFQ4</td>\n",
       "      <td>120401325X</td>\n",
       "      <td>JM</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>Item arrived in great time and was in perfect ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Cute!</td>\n",
       "      <td>1382313600</td>\n",
       "      <td>10 21, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATX7CZYFXI1KW</td>\n",
       "      <td>120401325X</td>\n",
       "      <td>patrice m rogoza</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>awesome! stays on, and looks great. can be use...</td>\n",
       "      <td>5</td>\n",
       "      <td>leopard home button sticker for iphone 4s</td>\n",
       "      <td>1359849600</td>\n",
       "      <td>02 3, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194434</th>\n",
       "      <td>A1YMNTFLNDYQ1F</td>\n",
       "      <td>B00LORXVUE</td>\n",
       "      <td>eyeused2loveher</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Works great just like my original one. I reall...</td>\n",
       "      <td>5</td>\n",
       "      <td>This works just perfect!</td>\n",
       "      <td>1405900800</td>\n",
       "      <td>07 21, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194435</th>\n",
       "      <td>A15TX8B2L8B20S</td>\n",
       "      <td>B00LORXVUE</td>\n",
       "      <td>Jon Davidson</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Great product. Great packaging. High quality a...</td>\n",
       "      <td>5</td>\n",
       "      <td>Great replacement cable. Apple certified</td>\n",
       "      <td>1405900800</td>\n",
       "      <td>07 21, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194436</th>\n",
       "      <td>A3JI7QRZO1QG8X</td>\n",
       "      <td>B00LORXVUE</td>\n",
       "      <td>Joyce M. Davidson</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This is a great cable, just as good as the mor...</td>\n",
       "      <td>5</td>\n",
       "      <td>Real quality</td>\n",
       "      <td>1405900800</td>\n",
       "      <td>07 21, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194437</th>\n",
       "      <td>A1NHB2VC68YQNM</td>\n",
       "      <td>B00LORXVUE</td>\n",
       "      <td>Nurse Farrugia</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I really like it becasue it works well with my...</td>\n",
       "      <td>5</td>\n",
       "      <td>I really like it becasue it works well with my...</td>\n",
       "      <td>1405814400</td>\n",
       "      <td>07 20, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194438</th>\n",
       "      <td>A1AG6U022WHXBF</td>\n",
       "      <td>B00LORXVUE</td>\n",
       "      <td>Trisha Crocker</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>product as described, I have wasted a lot of m...</td>\n",
       "      <td>5</td>\n",
       "      <td>I have wasted a lot of money on cords</td>\n",
       "      <td>1405900800</td>\n",
       "      <td>07 21, 2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194439 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            reviewerID        asin       reviewerName helpful  \\\n",
       "0       A30TL5EWN6DFXT  120401325X          christina  [0, 0]   \n",
       "1        ASY55RVNIL0UD  120401325X           emily l.  [0, 0]   \n",
       "2       A2TMXE2AFO7ONB  120401325X              Erica  [0, 0]   \n",
       "3        AWJ0WZQYMYFQ4  120401325X                 JM  [4, 4]   \n",
       "4        ATX7CZYFXI1KW  120401325X   patrice m rogoza  [2, 3]   \n",
       "...                ...         ...                ...     ...   \n",
       "194434  A1YMNTFLNDYQ1F  B00LORXVUE    eyeused2loveher  [0, 0]   \n",
       "194435  A15TX8B2L8B20S  B00LORXVUE       Jon Davidson  [0, 0]   \n",
       "194436  A3JI7QRZO1QG8X  B00LORXVUE  Joyce M. Davidson  [0, 0]   \n",
       "194437  A1NHB2VC68YQNM  B00LORXVUE     Nurse Farrugia  [0, 0]   \n",
       "194438  A1AG6U022WHXBF  B00LORXVUE     Trisha Crocker  [0, 0]   \n",
       "\n",
       "                                               reviewText  overall  \\\n",
       "0       They look good and stick good! I just don't li...        4   \n",
       "1       These stickers work like the review says they ...        5   \n",
       "2       These are awesome and make my phone look so st...        5   \n",
       "3       Item arrived in great time and was in perfect ...        4   \n",
       "4       awesome! stays on, and looks great. can be use...        5   \n",
       "...                                                   ...      ...   \n",
       "194434  Works great just like my original one. I reall...        5   \n",
       "194435  Great product. Great packaging. High quality a...        5   \n",
       "194436  This is a great cable, just as good as the mor...        5   \n",
       "194437  I really like it becasue it works well with my...        5   \n",
       "194438  product as described, I have wasted a lot of m...        5   \n",
       "\n",
       "                                                  summary  unixReviewTime  \\\n",
       "0                                              Looks Good      1400630400   \n",
       "1                                   Really great product.      1389657600   \n",
       "2                                          LOVE LOVE LOVE      1403740800   \n",
       "3                                                   Cute!      1382313600   \n",
       "4               leopard home button sticker for iphone 4s      1359849600   \n",
       "...                                                   ...             ...   \n",
       "194434                           This works just perfect!      1405900800   \n",
       "194435           Great replacement cable. Apple certified      1405900800   \n",
       "194436                                       Real quality      1405900800   \n",
       "194437  I really like it becasue it works well with my...      1405814400   \n",
       "194438              I have wasted a lot of money on cords      1405900800   \n",
       "\n",
       "         reviewTime  \n",
       "0       05 21, 2014  \n",
       "1       01 14, 2014  \n",
       "2       06 26, 2014  \n",
       "3       10 21, 2013  \n",
       "4        02 3, 2013  \n",
       "...             ...  \n",
       "194434  07 21, 2014  \n",
       "194435  07 21, 2014  \n",
       "194436  07 21, 2014  \n",
       "194437  07 20, 2014  \n",
       "194438  07 21, 2014  \n",
       "\n",
       "[194439 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194439, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text=df.reviewText.apply(gensim.utils.simple_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [they, look, good, and, stick, good, just, don...\n",
       "1         [these, stickers, work, like, the, review, say...\n",
       "2         [these, are, awesome, and, make, my, phone, lo...\n",
       "3         [item, arrived, in, great, time, and, was, in,...\n",
       "4         [awesome, stays, on, and, looks, great, can, b...\n",
       "                                ...                        \n",
       "194434    [works, great, just, like, my, original, one, ...\n",
       "194435    [great, product, great, packaging, high, quali...\n",
       "194436    [this, is, great, cable, just, as, good, as, t...\n",
       "194437    [really, like, it, becasue, it, works, well, w...\n",
       "194438    [product, as, described, have, wasted, lot, of...\n",
       "Name: reviewText, Length: 194439, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they',\n",
       " 'look',\n",
       " 'good',\n",
       " 'and',\n",
       " 'stick',\n",
       " 'good',\n",
       " 'just',\n",
       " 'don',\n",
       " 'like',\n",
       " 'the',\n",
       " 'rounded',\n",
       " 'shape',\n",
       " 'because',\n",
       " 'was',\n",
       " 'always',\n",
       " 'bumping',\n",
       " 'it',\n",
       " 'and',\n",
       " 'siri',\n",
       " 'kept',\n",
       " 'popping',\n",
       " 'up',\n",
       " 'and',\n",
       " 'it',\n",
       " 'was',\n",
       " 'irritating',\n",
       " 'just',\n",
       " 'won',\n",
       " 'buy',\n",
       " 'product',\n",
       " 'like',\n",
       " 'this',\n",
       " 'again']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_text.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"They look good and stick good! I just don't like the rounded shape because I was always bumping it and Siri kept popping up and it was irritating. I just won't buy a product like this again\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviewText.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training word 2 vector model\n",
    "\n",
    "#Train the model for reviews. Use a window of size 10 i.e. 10 words before the present word and 10 words ahead. A sentence with at least 2 words should only be considered, configure this using min_count parameter.\n",
    "\n",
    "#Workers define how many CPU threads to be used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the model\n",
    "model = gensim.models.Word2Vec(window=10,min_count=2,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Vocabulary\n",
    "\n",
    "model.build_vocab(review_text, progress_per=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61505105, 83868975)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the word2vec model\n",
    "\n",
    "model.train(review_text,total_examples=model.corpus_count,epochs= model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "model.save(\"./word2vec-amazon-cell-accessories-reviews-short.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('experiences', 0.7549813985824585),\n",
       " ('success', 0.5746661424636841),\n",
       " ('results', 0.5310482382774353),\n",
       " ('dissatisfaction', 0.5005948543548584),\n",
       " ('respect', 0.498167484998703),\n",
       " ('novice', 0.490531325340271),\n",
       " ('perspective', 0.48767054080963135),\n",
       " ('opinion', 0.48721110820770264),\n",
       " ('faith', 0.4846615791320801),\n",
       " ('impressions', 0.4831383526325226)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding similar words and similarity between words\n",
    "\n",
    "model.wv.most_similar(\"experience\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4406756"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.wv.similarity(w1=\"cheap\",w2=\"inexpensive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7207874"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(w1=\"great\", w2=\"perfect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
